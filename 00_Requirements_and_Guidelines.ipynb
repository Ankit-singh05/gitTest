{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd3b7719-01a1-4d59-9496-0f8c6b0c4455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Requirements and Guidelines\n",
    "\n",
    "This notebook serves as a **requirements document** and **guideline** for implementing Bronze, Silver, and Gold layer transformations.\n",
    "\n",
    "**Use this notebook as a reference** when building your data transformation pipelines.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Architecture Overview](#architecture-overview)\n",
    "2. [Bronze Layer Requirements](#bronze-layer-requirements)\n",
    "3. [Silver Layer Requirements](#silver-layer-requirements)\n",
    "4. [Gold Layer Requirements](#gold-layer-requirements)\n",
    "5. [Best Practices](#best-practices)\n",
    "6. [Checkpoints and Validation](#checkpoints-and-validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067dd8d3-a077-404e-9efc-0d5011896011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Architecture Overview\n",
    "\n",
    "### Medallion Architecture Pattern\n",
    "\n",
    "```\n",
    "┌─────────────────┐\n",
    "│   Raw Data      │  CSV/Parquet files with data quality issues\n",
    "│   (raw_data)    │  - Multiple date formats\n",
    "└────────┬────────┘  - Missing/null values\n",
    "         │           - Invalid data types\n",
    "         │           - Bad records (negative, out of range)\n",
    "         ▼\n",
    "┌─────────────────┐\n",
    "│  Bronze Layer   │  Cleaned and standardized data\n",
    "│   (bronze)      │  - Standardized formats\n",
    "└────────┬────────┘  - Basic data type conversions\n",
    "         │           - Data quality flags\n",
    "         │           - Metadata tracking\n",
    "         ▼\n",
    "┌─────────────────┐\n",
    "│  Silver Layer   │  Transformed and enriched data\n",
    "│   (silver)      │  - Business logic applied\n",
    "└────────┬────────┘  - Data enrichment (joins)\n",
    "         │           - Derived fields calculated\n",
    "         │           - Single source of truth\n",
    "         ▼\n",
    "┌─────────────────┐\n",
    "│   Gold Layer    │  Curated and aggregated data\n",
    "│    (gold)       │  - Pre-aggregated metrics\n",
    "└─────────────────┘  - Analytics-ready datasets\n",
    "                     - Business KPIs\n",
    "```\n",
    "\n",
    "### Key Principles\n",
    "\n",
    "1. **ELT Approach**: Extract → Load → Transform (not ETL)\n",
    "2. **Data Preservation**: Bronze layer preserves raw data structure\n",
    "3. **Incremental Processing**: Each layer builds upon the previous\n",
    "4. **Delta Tables**: Use Delta format for ACID transactions and time travel\n",
    "5. **Partitioning**: Partition large tables for performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03566d14-8a11-4e25-9330-7e46e5d81224",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bronze Layer Requirements\n",
    "\n",
    "### Purpose\n",
    "The Bronze layer is the **first landing zone** for raw data. It focuses on:\n",
    "- Initial data ingestion\n",
    "- Basic cleaning and standardization\n",
    "- Preserving raw data structure\n",
    "- Adding metadata and quality flags\n",
    "\n",
    "### Required Tasks\n",
    "\n",
    "#### 1. Data Loading\n",
    "- [ ] Load data from `raw_data` volume\n",
    "- [ ] Handle multiple file formats (CSV, Parquet)\n",
    "- [ ] Handle partitioned data (multiple files)\n",
    "- [ ] Preserve source file information\n",
    "\n",
    "#### 2. Date Standardization\n",
    "- [ ] Handle multiple date formats:\n",
    "  - `YYYY-MM-DD` (e.g., 2023-12-25)\n",
    "  - `MM/DD/YYYY` (e.g., 12/25/2023)\n",
    "  - `DD-MM-YYYY` (e.g., 25-12-2023)\n",
    "  - `YYYYMMDD` (e.g., 20231225)\n",
    "- [ ] Convert invalid dates to NULL\n",
    "- [ ] Use `F.coalesce()` with multiple `F.to_date()` attempts\n",
    "\n",
    "**Example Pattern:**\n",
    "```python\n",
    "df = df.withColumn(\n",
    "    \"order_date\",\n",
    "    F.coalesce(\n",
    "        F.to_date(F.col(\"order_date\"), \"yyyy-MM-dd\"),\n",
    "        F.to_date(F.col(\"order_date\"), \"MM/dd/yyyy\"),\n",
    "        F.to_date(F.col(\"order_date\"), \"dd-MM-yyyy\"),\n",
    "        F.to_date(F.col(\"order_date\"), \"yyyyMMdd\"),\n",
    "        F.lit(None).cast(DateType())\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "#### 3. Data Type Conversions\n",
    "- [ ] Convert string IDs to integers (customer_id, product_id, etc.)\n",
    "- [ ] Convert numeric strings to appropriate types (double, int)\n",
    "- [ ] Ensure date columns are DateType\n",
    "- [ ] Handle type conversion errors gracefully\n",
    "\n",
    "#### 4. Basic Data Cleaning\n",
    "- [ ] Handle negative values (set to NULL or flag)\n",
    "- [ ] Handle out-of-range values (discounts > 100%, etc.)\n",
    "- [ ] Replace obviously invalid values with NULL\n",
    "- [ ] **DO NOT** apply business logic yet (save for Silver)\n",
    "\n",
    "#### 5. Missing Value Handling\n",
    "- [ ] Identify columns with null values\n",
    "- [ ] Document null percentages\n",
    "- [ ] Add quality flags for nulls\n",
    "- [ ] **DO NOT** fill missing values yet (save for Silver)\n",
    "\n",
    "#### 6. Metadata Addition\n",
    "- [ ] Add `ingestion_timestamp` column\n",
    "- [ ] Add `source_file` column (track where data came from)\n",
    "- [ ] Add data quality flags:\n",
    "  - `has_invalid_date`\n",
    "  - `has_negative_quantity`\n",
    "  - `has_invalid_discount`\n",
    "  - `has_null_customer`\n",
    "  - etc.\n",
    "\n",
    "#### 7. Data Quality Checks\n",
    "- [ ] Count records with data quality issues\n",
    "- [ ] Report null percentages\n",
    "- [ ] Identify bad records\n",
    "- [ ] Log quality metrics\n",
    "\n",
    "#### 8. Save to Bronze\n",
    "- [ ] Save as Delta table format\n",
    "- [ ] Use appropriate partitioning (e.g., by date for orders)\n",
    "- [ ] Use `overwriteSchema` option for schema evolution\n",
    "- [ ] Save to `/Volumes/databricks_training/delta_demo/bronze/{table_name}`\n",
    "\n",
    "### Bronze Layer Checklist\n",
    "\n",
    "For each table (geographies, products, customers, orders):\n",
    "\n",
    "- [ ] Data loaded successfully\n",
    "- [ ] Date columns standardized\n",
    "- [ ] Data types converted correctly\n",
    "- [ ] Invalid values handled (set to NULL)\n",
    "- [ ] Metadata columns added\n",
    "- [ ] Quality flags added\n",
    "- [ ] Data quality summary generated\n",
    "- [ ] Saved as Delta table in bronze volume\n",
    "- [ ] Record count verified\n",
    "- [ ] Schema validated\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "**Bronze Tables:**\n",
    "- `bronze/geographies` - Cleaned geography reference data\n",
    "- `bronze/products` - Cleaned product catalog\n",
    "- `bronze/customers` - Cleaned customer master data\n",
    "- `bronze/orders` - Cleaned order transactions (partitioned by order_date)\n",
    "\n",
    "**Key Characteristics:**\n",
    "- Data is cleaned but not transformed\n",
    "- Original structure preserved\n",
    "- Quality issues identified and flagged\n",
    "- Ready for Silver layer transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c880110-3953-455e-82f4-751ae09d0035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver Layer Requirements\n",
    "\n",
    "### Purpose\n",
    "The Silver layer applies **business transformations** and creates a **single source of truth** for analytics. It focuses on:\n",
    "- Business logic application\n",
    "- Data enrichment through joins\n",
    "- Handling missing values with business rules\n",
    "- Calculating derived fields\n",
    "- Creating analytics-ready datasets\n",
    "\n",
    "### Required Tasks\n",
    "\n",
    "#### 1. Load Bronze Data\n",
    "- [ ] Load all bronze tables\n",
    "- [ ] Verify record counts match bronze layer\n",
    "- [ ] Check data quality flags from bronze\n",
    "\n",
    "#### 2. Reference Tables (Geographies, Products)\n",
    "- [ ] **Geographies**: Minimal transformation (already clean)\n",
    "  - [ ] Ensure no null geography_ids\n",
    "  - [ ] Standardize postal codes\n",
    "  - [ ] Add updated_timestamp\n",
    "\n",
    "- [ ] **Products**: Apply business rules\n",
    "  - [ ] Handle missing prices: Use `cost * 1.5` if price is NULL\n",
    "  - [ ] Handle missing categories: Set to \"UNKNOWN\"\n",
    "  - [ ] Calculate `profit_margin = (price - cost) / price * 100`\n",
    "  - [ ] Handle missing supplier_id: Set to 0\n",
    "  - [ ] Remove invalid products (null product_id)\n",
    "\n",
    "#### 3. Dimension Tables (Customers)\n",
    "- [ ] Enrich with geography information (JOIN)\n",
    "- [ ] Create `full_name = first_name + \" \" + last_name`\n",
    "- [ ] Handle missing geography_id: Set to 0\n",
    "- [ ] Handle missing geography info: Set country/state/city to \"UNKNOWN\"\n",
    "- [ ] Handle missing registration_date: Set default date (e.g., 2020-01-01)\n",
    "- [ ] Remove invalid customers (null customer_id)\n",
    "\n",
    "#### 4. Fact Table (Orders)\n",
    "- [ ] Enrich with product information (JOIN)\n",
    "  - [ ] Get product_name, category, price, cost\n",
    "- [ ] Enrich with customer information (JOIN)\n",
    "  - [ ] Get full_name, country, state, city\n",
    "- [ ] Apply business rules for missing values:\n",
    "  - [ ] `quantity`: Default to 1 if NULL\n",
    "  - [ ] `unit_price`: Use product price if NULL\n",
    "  - [ ] `discount`: Default to 0.0 if NULL, cap at 1.0 (100%)\n",
    "  - [ ] `shipping_cost`: Default to 0.0 if NULL\n",
    "  - [ ] `order_date`: Use current_date() if NULL\n",
    "  - [ ] `customer_id`: Set to 0 if NULL\n",
    "  - [ ] `product_id`: Set to 0 if NULL\n",
    "\n",
    "#### 5. Calculate Derived Fields (Orders)\n",
    "- [ ] `line_total = quantity * unit_price`\n",
    "- [ ] `discount_amount = line_total * discount`\n",
    "- [ ] `net_amount = line_total - discount_amount`\n",
    "- [ ] `total_amount = net_amount + shipping_cost`\n",
    "- [ ] `profit = (unit_price - cost) * quantity`\n",
    "- [ ] `year = year(order_date)`\n",
    "- [ ] `month = month(order_date)`\n",
    "- [ ] `quarter = quarter(order_date)`\n",
    "\n",
    "#### 6. Data Validation\n",
    "- [ ] Ensure all required fields are populated\n",
    "- [ ] Verify calculated fields are correct\n",
    "- [ ] Check for data integrity (foreign keys, etc.)\n",
    "- [ ] Validate business rules are applied\n",
    "\n",
    "#### 7. Save to Silver\n",
    "- [ ] Save as Delta table format\n",
    "- [ ] Use appropriate partitioning (orders by order_date)\n",
    "- [ ] Use `overwriteSchema` option\n",
    "- [ ] Save to `/Volumes/databricks_training/delta_demo/silver/{table_name}`\n",
    "\n",
    "### Silver Layer Checklist\n",
    "\n",
    "For each table:\n",
    "\n",
    "**Geographies:**\n",
    "- [ ] Loaded from bronze\n",
    "- [ ] Null values handled\n",
    "- [ ] Saved to silver\n",
    "\n",
    "**Products:**\n",
    "- [ ] Missing prices handled (cost * 1.5)\n",
    "- [ ] Profit margin calculated\n",
    "- [ ] Missing categories handled\n",
    "- [ ] Saved to silver\n",
    "\n",
    "**Customers:**\n",
    "- [ ] Enriched with geography (JOIN)\n",
    "- [ ] Full name created\n",
    "- [ ] Missing values handled with defaults\n",
    "- [ ] Saved to silver\n",
    "\n",
    "**Orders:**\n",
    "- [ ] Enriched with product (JOIN)\n",
    "- [ ] Enriched with customer (JOIN)\n",
    "- [ ] Missing values handled with business rules\n",
    "- [ ] Derived fields calculated:\n",
    "  - [ ] line_total\n",
    "  - [ ] discount_amount\n",
    "  - [ ] net_amount\n",
    "  - [ ] total_amount\n",
    "  - [ ] profit\n",
    "  - [ ] year, month, quarter\n",
    "- [ ] Saved to silver (partitioned by order_date)\n",
    "- [ ] Total revenue verified\n",
    "- [ ] Total profit verified\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "**Silver Tables:**\n",
    "- `silver/geographies` - Reference table\n",
    "- `silver/products` - Products with profit_margin\n",
    "- `silver/customers` - Customers enriched with geography\n",
    "- `silver/orders` - Orders enriched with product and customer, all calculated fields\n",
    "\n",
    "**Key Characteristics:**\n",
    "- Business logic applied\n",
    "- Data enriched through joins\n",
    "- Missing values handled with business rules\n",
    "- Derived fields calculated\n",
    "- Single source of truth for analytics\n",
    "- Ready for Gold layer aggregations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efa6c8ed-8507-426d-903f-366a10f2773c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Gold Layer Requirements\n",
    "\n",
    "### Purpose\n",
    "The Gold layer creates **curated and aggregated datasets** optimized for analytics and reporting. It focuses on:\n",
    "- Pre-aggregated metrics\n",
    "- Business KPIs\n",
    "- Analytics-ready datasets\n",
    "- Performance optimization\n",
    "\n",
    "### Required Tasks\n",
    "\n",
    "#### 1. Load Silver Data\n",
    "- [ ] Load all silver tables\n",
    "- [ ] Verify data quality\n",
    "- [ ] Check record counts\n",
    "\n",
    "#### 2. Create Aggregated Tables\n",
    "\n",
    "**Table 1: Sales Summary by Date**\n",
    "- [ ] Group by: `order_date`, `year`, `month`, `quarter`\n",
    "- [ ] Aggregations:\n",
    "  - [ ] `total_orders` (count of order_id)\n",
    "  - [ ] `total_quantity` (sum of quantity)\n",
    "  - [ ] `total_revenue` (sum of total_amount)\n",
    "  - [ ] `total_discount` (sum of discount_amount)\n",
    "  - [ ] `total_shipping` (sum of shipping_cost)\n",
    "  - [ ] `total_profit` (sum of profit)\n",
    "  - [ ] `avg_order_value` (avg of total_amount)\n",
    "  - [ ] `unique_customers` (count distinct customer_id)\n",
    "  - [ ] `unique_products` (count distinct product_id)\n",
    "- [ ] Calculated fields:\n",
    "  - [ ] `profit_margin_pct = (total_profit / total_revenue) * 100`\n",
    "- [ ] Order by: `order_date`\n",
    "- [ ] Save to: `gold/sales_by_date`\n",
    "\n",
    "**Table 2: Sales Summary by Product Category**\n",
    "- [ ] Group by: `category`\n",
    "- [ ] Aggregations:\n",
    "  - [ ] `total_orders`\n",
    "  - [ ] `total_quantity_sold`\n",
    "  - [ ] `total_revenue`\n",
    "  - [ ] `total_profit`\n",
    "  - [ ] `avg_order_value`\n",
    "  - [ ] `avg_unit_price`\n",
    "  - [ ] `unique_products`\n",
    "  - [ ] `unique_customers`\n",
    "- [ ] Calculated fields:\n",
    "  - [ ] `profit_margin_pct`\n",
    "  - [ ] `revenue_share_pct = (category_revenue / total_revenue) * 100`\n",
    "- [ ] Order by: `total_revenue DESC`\n",
    "- [ ] Save to: `gold/sales_by_category`\n",
    "\n",
    "**Table 3: Sales Summary by Geography**\n",
    "- [ ] Group by: `country`, `state`, `city`\n",
    "- [ ] Aggregations:\n",
    "  - [ ] `total_orders`\n",
    "  - [ ] `total_quantity_sold`\n",
    "  - [ ] `total_revenue`\n",
    "  - [ ] `total_profit`\n",
    "  - [ ] `avg_order_value`\n",
    "  - [ ] `unique_customers`\n",
    "  - [ ] `unique_products`\n",
    "- [ ] Calculated fields:\n",
    "  - [ ] `profit_margin_pct`\n",
    "- [ ] Order by: `total_revenue DESC`\n",
    "- [ ] Save to: `gold/sales_by_geography`\n",
    "\n",
    "**Table 4: Customer Analytics**\n",
    "- [ ] Group by: `customer_id`, `full_name`, `country`, `state`, `city`\n",
    "- [ ] Aggregations:\n",
    "  - [ ] `total_orders`\n",
    "  - [ ] `total_items_purchased` (sum of quantity)\n",
    "  - [ ] `lifetime_value` (sum of total_amount)\n",
    "  - [ ] `avg_order_value`\n",
    "  - [ ] `first_order_date` (min of order_date)\n",
    "  - [ ] `last_order_date` (max of order_date)\n",
    "  - [ ] `unique_products_purchased`\n",
    "  - [ ] `unique_categories_purchased`\n",
    "- [ ] Calculated fields:\n",
    "  - [ ] `days_since_first_order`\n",
    "  - [ ] `days_since_last_order`\n",
    "  - [ ] `avg_days_between_orders`\n",
    "  - [ ] `customer_segment`:\n",
    "    - VIP: lifetime_value >= 10000\n",
    "    - Premium: lifetime_value >= 5000\n",
    "    - Regular: lifetime_value >= 1000\n",
    "    - New: otherwise\n",
    "- [ ] Order by: `lifetime_value DESC`\n",
    "- [ ] Save to: `gold/customer_analytics`\n",
    "\n",
    "**Table 5: Product Performance**\n",
    "- [ ] Group by: `product_id`, `product_name`, `category`\n",
    "- [ ] Aggregations:\n",
    "  - [ ] `total_orders`\n",
    "  - [ ] `total_quantity_sold`\n",
    "  - [ ] `total_revenue`\n",
    "  - [ ] `total_profit`\n",
    "  - [ ] `avg_selling_price`\n",
    "  - [ ] `min_price`\n",
    "  - [ ] `max_price`\n",
    "  - [ ] `unique_customers`\n",
    "- [ ] Calculated fields:\n",
    "  - [ ] `profit_margin_pct`\n",
    "  - [ ] `avg_quantity_per_order`\n",
    "  - [ ] `product_rank` (dense_rank by total_revenue)\n",
    "- [ ] Order by: `total_revenue DESC`\n",
    "- [ ] Save to: `gold/product_performance`\n",
    "\n",
    "**Table 6: Monthly Sales Trend**\n",
    "- [ ] Group by: `year`, `month`, `quarter`\n",
    "- [ ] Aggregations:\n",
    "  - [ ] `total_orders`\n",
    "  - [ ] `total_quantity`\n",
    "  - [ ] `total_revenue`\n",
    "  - [ ] `total_profit`\n",
    "  - [ ] `avg_order_value`\n",
    "  - [ ] `unique_customers`\n",
    "  - [ ] `unique_products`\n",
    "- [ ] Calculated fields:\n",
    "  - [ ] `profit_margin_pct`\n",
    "  - [ ] `month_name` (January, February, etc.)\n",
    "- [ ] Order by: `year`, `month`\n",
    "- [ ] Save to: `gold/monthly_sales_trend`\n",
    "\n",
    "#### 3. Data Validation\n",
    "- [ ] Verify aggregation calculations\n",
    "- [ ] Check for null values in key metrics\n",
    "- [ ] Validate business logic (segments, ranks, etc.)\n",
    "- [ ] Compare totals match silver layer\n",
    "\n",
    "#### 4. Save to Gold\n",
    "- [ ] Save all tables as Delta format\n",
    "- [ ] No partitioning needed (already aggregated)\n",
    "- [ ] Use `overwriteSchema` option\n",
    "- [ ] Save to `/Volumes/databricks_training/delta_demo/gold/{table_name}`\n",
    "\n",
    "### Gold Layer Checklist\n",
    "\n",
    "- [ ] **Sales by Date**\n",
    "  - [ ] All aggregations calculated\n",
    "  - [ ] Profit margin calculated\n",
    "  - [ ] Saved to gold\n",
    "\n",
    "- [ ] **Sales by Category**\n",
    "  - [ ] All aggregations calculated\n",
    "  - [ ] Revenue share calculated\n",
    "  - [ ] Saved to gold\n",
    "\n",
    "- [ ] **Sales by Geography**\n",
    "  - [ ] All aggregations calculated\n",
    "  - [ ] Saved to gold\n",
    "\n",
    "- [ ] **Customer Analytics**\n",
    "  - [ ] Lifetime value calculated\n",
    "  - [ ] Customer segments assigned\n",
    "  - [ ] Date calculations correct\n",
    "  - [ ] Saved to gold\n",
    "\n",
    "- [ ] **Product Performance**\n",
    "  - [ ] All aggregations calculated\n",
    "  - [ ] Product rank calculated\n",
    "  - [ ] Saved to gold\n",
    "\n",
    "- [ ] **Monthly Sales Trend**\n",
    "  - [ ] All aggregations calculated\n",
    "  - [ ] Month names added\n",
    "  - [ ] Saved to gold\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "**Gold Tables:**\n",
    "- `gold/sales_by_date` - Daily sales metrics\n",
    "- `gold/sales_by_category` - Category performance\n",
    "- `gold/sales_by_geography` - Geographic performance\n",
    "- `gold/customer_analytics` - Customer insights and segments\n",
    "- `gold/product_performance` - Product rankings and metrics\n",
    "- `gold/monthly_sales_trend` - Monthly trends\n",
    "\n",
    "**Key Characteristics:**\n",
    "- Pre-aggregated for fast queries\n",
    "- Business KPIs included\n",
    "- Analytics-ready\n",
    "- Optimized for reporting tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33146659-77ad-4bc5-baa4-fb4f3008385b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Code Organization\n",
    "- Use clear variable names: `df_{table}_{layer}`\n",
    "- Add comments explaining business logic\n",
    "- Break complex transformations into steps\n",
    "- Use intermediate variables for readability\n",
    "\n",
    "### 2. Error Handling\n",
    "- Always check for null values before operations\n",
    "- Use `F.coalesce()` for safe null handling\n",
    "- Validate data types before casting\n",
    "- Handle division by zero (use `F.when()` conditions)\n",
    "\n",
    "### 3. Performance Optimization\n",
    "- Use appropriate partitioning (by date for time-series data)\n",
    "- Avoid unnecessary shuffles\n",
    "- Use broadcast joins for small reference tables\n",
    "- Cache intermediate results if reused\n",
    "\n",
    "### 4. Data Quality\n",
    "- Always add quality flags\n",
    "- Track data lineage (source_file, ingestion_timestamp)\n",
    "- Validate record counts between layers\n",
    "- Document assumptions and business rules\n",
    "\n",
    "### 5. Delta Table Best Practices\n",
    "- Always use `.format(\"delta\")`\n",
    "- Use `.mode(\"overwrite\")` for full refresh\n",
    "- Use `.option(\"overwriteSchema\", \"true\")` for schema evolution\n",
    "- Partition large tables appropriately\n",
    "- Use `F.current_timestamp()` for audit columns\n",
    "\n",
    "### 6. Transformation Patterns\n",
    "\n",
    "**Pattern 1: Safe Null Handling**\n",
    "```python\n",
    "df = df.withColumn(\"column\", F.coalesce(F.col(\"column\"), F.lit(default_value)))\n",
    "```\n",
    "\n",
    "**Pattern 2: Conditional Logic**\n",
    "```python\n",
    "df = df.withColumn(\"segment\",\n",
    "    F.when(F.col(\"value\") >= 10000, \"VIP\")\n",
    "     .when(F.col(\"value\") >= 5000, \"Premium\")\n",
    "     .otherwise(\"Regular\"))\n",
    "```\n",
    "\n",
    "**Pattern 3: Calculated Fields**\n",
    "```python\n",
    "df = df.withColumn(\"total\", \n",
    "    F.col(\"quantity\") * F.col(\"unit_price\"))\n",
    "```\n",
    "\n",
    "**Pattern 4: Aggregations**\n",
    "```python\n",
    "df_agg = df.groupBy(\"category\").agg(\n",
    "    F.count(\"order_id\").alias(\"total_orders\"),\n",
    "    F.sum(\"revenue\").alias(\"total_revenue\")\n",
    ")\n",
    "```\n",
    "\n",
    "### 7. Common Pitfalls to Avoid\n",
    "\n",
    "❌ **Don't:**\n",
    "- Apply business logic in Bronze layer\n",
    "- Fill missing values in Bronze (just flag them)\n",
    "- Hard-code values (use variables/constants)\n",
    "- Skip data quality checks\n",
    "- Forget to handle nulls in calculations\n",
    "- Use string concatenation for dates\n",
    "\n",
    "✅ **Do:**\n",
    "- Preserve raw data structure in Bronze\n",
    "- Apply business rules in Silver\n",
    "- Use `F.coalesce()` for null handling\n",
    "- Add metadata and quality flags\n",
    "- Validate calculations\n",
    "- Use proper date functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91ff56f5-0fec-49f4-94e3-962561cb0015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Checkpoints and Validation\n",
    "\n",
    "### After Bronze Layer\n",
    "\n",
    "Run these checks:\n",
    "\n",
    "```python\n",
    "# 1. Verify all tables exist\n",
    "tables = [\"geographies\", \"products\", \"customers\", \"orders\"]\n",
    "for table in tables:\n",
    "    df = spark.read.format(\"delta\").load(f\"{bronze_path}/{table}\")\n",
    "    print(f\"{table}: {df.count():,} records\")\n",
    "\n",
    "# 2. Check data quality flags\n",
    "df_orders = spark.read.format(\"delta\").load(f\"{bronze_path}/orders\")\n",
    "print(\"Data Quality Issues:\")\n",
    "print(f\"Invalid dates: {df_orders.filter(F.col('has_invalid_date')).count():,}\")\n",
    "print(f\"Negative quantities: {df_orders.filter(F.col('has_negative_quantity')).count():,}\")\n",
    "\n",
    "# 3. Check null percentages\n",
    "for col in df_orders.columns:\n",
    "    null_count = df_orders.filter(F.col(col).isNull()).count()\n",
    "    null_pct = (null_count / df_orders.count()) * 100\n",
    "    if null_pct > 0:\n",
    "        print(f\"{col}: {null_pct:.2f}% null\")\n",
    "```\n",
    "\n",
    "### After Silver Layer\n",
    "\n",
    "Run these checks:\n",
    "\n",
    "```python\n",
    "# 1. Verify all tables exist\n",
    "tables = [\"geographies\", \"products\", \"customers\", \"orders\"]\n",
    "for table in tables:\n",
    "    df = spark.read.format(\"delta\").load(f\"{silver_path}/{table}\")\n",
    "    print(f\"{table}: {df.count():,} records\")\n",
    "\n",
    "# 2. Verify business logic\n",
    "df_orders = spark.read.format(\"delta\").load(f\"{silver_path}/orders\")\n",
    "print(\"Business Logic Validation:\")\n",
    "print(f\"Total Revenue: ${df_orders.agg(F.sum('total_amount')).collect()[0][0]:,.2f}\")\n",
    "print(f\"Total Profit: ${df_orders.agg(F.sum('profit')).collect()[0][0]:,.2f}\")\n",
    "\n",
    "# 3. Check for nulls in key fields\n",
    "key_fields = [\"customer_id\", \"product_id\", \"order_date\", \"total_amount\"]\n",
    "for field in key_fields:\n",
    "    null_count = df_orders.filter(F.col(field).isNull()).count()\n",
    "    print(f\"{field} nulls: {null_count}\")\n",
    "\n",
    "# 4. Verify calculated fields\n",
    "df_orders.select(\"line_total\", \"discount_amount\", \"net_amount\", \"total_amount\").show(10)\n",
    "```\n",
    "\n",
    "### After Gold Layer\n",
    "\n",
    "Run these checks:\n",
    "\n",
    "```python\n",
    "# 1. Verify all tables exist\n",
    "gold_tables = [\n",
    "    \"sales_by_date\",\n",
    "    \"sales_by_category\",\n",
    "    \"sales_by_geography\",\n",
    "    \"customer_analytics\",\n",
    "    \"product_performance\",\n",
    "    \"monthly_sales_trend\"\n",
    "]\n",
    "\n",
    "for table in gold_tables:\n",
    "    df = spark.read.format(\"delta\").load(f\"{gold_path}/{table}\")\n",
    "    print(f\"{table}: {df.count():,} records\")\n",
    "\n",
    "# 2. Verify aggregations match silver\n",
    "df_orders_silver = spark.read.format(\"delta\").load(f\"{silver_path}/orders\")\n",
    "df_sales_by_date = spark.read.format(\"delta\").load(f\"{gold_path}/sales_by_date\")\n",
    "\n",
    "silver_total = df_orders_silver.agg(F.sum(\"total_amount\")).collect()[0][0]\n",
    "gold_total = df_sales_by_date.agg(F.sum(\"total_revenue\")).collect()[0][0]\n",
    "\n",
    "print(f\"Silver total revenue: ${silver_total:,.2f}\")\n",
    "print(f\"Gold total revenue: ${gold_total:,.2f}\")\n",
    "print(f\"Match: {abs(silver_total - gold_total) < 0.01}\")\n",
    "\n",
    "# 3. Sample each table\n",
    "for table in gold_tables:\n",
    "    df = spark.read.format(\"delta\").load(f\"{gold_path}/{table}\")\n",
    "    print(f\"\\n{table} sample:\")\n",
    "    df.show(5, truncate=False)\n",
    "```\n",
    "\n",
    "### Success Criteria\n",
    "\n",
    "✅ **Bronze Layer is successful if:**\n",
    "- All 4 tables created\n",
    "- Data types are correct\n",
    "- Dates are standardized\n",
    "- Quality flags are present\n",
    "- No data loss (record counts reasonable)\n",
    "\n",
    "✅ **Silver Layer is successful if:**\n",
    "- All 4 tables created\n",
    "- Business logic applied correctly\n",
    "- Joins successful (no unexpected nulls)\n",
    "- Calculated fields are correct\n",
    "- Total revenue/profit are reasonable\n",
    "\n",
    "✅ **Gold Layer is successful if:**\n",
    "- All 6 tables created\n",
    "- Aggregations match silver totals\n",
    "- Business metrics are calculated\n",
    "- Customer segments assigned\n",
    "- Product ranks calculated\n",
    "- Ready for analytics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99663343-af50-4dce-add2-09bcd9e62d60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Quick Reference\n",
    "\n",
    "### Paths\n",
    "```python\n",
    "raw_data_path = \"/Volumes/databricks_training/delta_demo/raw_data\"\n",
    "bronze_path = \"/Volumes/databricks_training/delta_demo/bronze\"\n",
    "silver_path = \"/Volumes/databricks_training/delta_demo/silver\"\n",
    "gold_path = \"/Volumes/databricks_training/delta_demo/gold\"\n",
    "```\n",
    "\n",
    "### Common Imports\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "```\n",
    "\n",
    "### Standard Delta Write Pattern\n",
    "```python\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .partitionBy(\"order_date\") \\  # Optional, for large tables\n",
    "    .save(f\"{path}/{table_name}\")\n",
    "```\n",
    "\n",
    "### Standard Delta Read Pattern\n",
    "```python\n",
    "df = spark.read.format(\"delta\").load(f\"{path}/{table_name}\")\n",
    "```\n",
    "\n",
    "### Date Standardization Pattern\n",
    "```python\n",
    "df = df.withColumn(\n",
    "    \"date_column\",\n",
    "    F.coalesce(\n",
    "        F.to_date(F.col(\"date_column\"), \"yyyy-MM-dd\"),\n",
    "        F.to_date(F.col(\"date_column\"), \"MM/dd/yyyy\"),\n",
    "        F.to_date(F.col(\"date_column\"), \"dd-MM-yyyy\"),\n",
    "        F.to_date(F.col(\"date_column\"), \"yyyyMMdd\"),\n",
    "        F.lit(None).cast(DateType())\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "### Null Handling Pattern\n",
    "```python\n",
    "# Default value\n",
    "df = df.withColumn(\"column\", F.coalesce(F.col(\"column\"), F.lit(\"UNKNOWN\")))\n",
    "\n",
    "# Business rule\n",
    "df = df.withColumn(\"price\", F.coalesce(F.col(\"price\"), F.col(\"cost\") * 1.5))\n",
    "```\n",
    "\n",
    "### Aggregation Pattern\n",
    "```python\n",
    "df_agg = df.groupBy(\"group_column\").agg(\n",
    "    F.count(\"id\").alias(\"count\"),\n",
    "    F.sum(\"amount\").alias(\"total\"),\n",
    "    F.avg(\"amount\").alias(\"average\"),\n",
    "    F.countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Table Registration for Dashboards\n",
    "\n",
    "### Why Register Tables?\n",
    "\n",
    "When you save data as Delta format to volumes (using `.save()`), the data is stored as **files**. To use them in:\n",
    "- **Databricks SQL** queries\n",
    "- **Databricks Dashboards**\n",
    "- **SQL Analytics**\n",
    "- **BI Tools** (Tableau, Power BI, etc.)\n",
    "\n",
    "You **MUST register them as tables** in Unity Catalog.\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always Register Gold Tables** ✅\n",
    "   - Gold tables are optimized for dashboards\n",
    "   - Pre-aggregated = fast queries\n",
    "   - Primary source for analytics\n",
    "\n",
    "2. **Register Silver Tables** ✅\n",
    "   - For ad-hoc SQL queries\n",
    "   - Complex dashboards needing detailed data\n",
    "   - Data exploration\n",
    "\n",
    "3. **Optional: Bronze Tables** ⚠️\n",
    "   - Only if needed for data quality monitoring\n",
    "   - Usually not needed for dashboards\n",
    "\n",
    "4. **Create Views** ✅\n",
    "   - For specific dashboard queries\n",
    "   - Pre-filtered data (e.g., current year)\n",
    "   - Simplified schemas\n",
    "   - Security (hide sensitive columns)\n",
    "\n",
    "### How to Register Tables\n",
    "\n",
    "**Pattern:**\n",
    "```python\n",
    "# Register a Delta table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS databricks_training.delta_demo.gold_{table_name}\n",
    "    USING DELTA\n",
    "    LOCATION '/Volumes/databricks_training/delta_demo/gold/{table_name}'\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "**After Registration:**\n",
    "- Tables are accessible via SQL: `SELECT * FROM databricks_training.delta_demo.gold_sales_by_date`\n",
    "- Can be used in Databricks SQL and Dashboards\n",
    "- Can be connected to BI tools\n",
    "\n",
    "### When to Create Views\n",
    "\n",
    "Create views when you need:\n",
    "- **Filtered data**: `WHERE year = 2024`\n",
    "- **Simplified schema**: Only selected columns\n",
    "- **Combined metrics**: Joins from multiple tables\n",
    "- **Security**: Hide sensitive columns\n",
    "\n",
    "**Pattern:**\n",
    "```python\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW databricks_training.delta_demo.v_current_year_sales AS\n",
    "    SELECT * FROM databricks_training.delta_demo.gold_sales_by_date\n",
    "    WHERE year = YEAR(CURRENT_DATE())\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "### Checklist for Table Registration\n",
    "\n",
    "After creating Gold layer tables:\n",
    "- [ ] Register all Gold tables in Unity Catalog\n",
    "- [ ] Verify tables are accessible via SQL\n",
    "- [ ] Create views for common dashboard queries\n",
    "- [ ] Test queries in Databricks SQL\n",
    "- [ ] Document table names and purposes\n",
    "\n",
    "**See `06_Register_Tables_for_Dashboards.ipynb` for complete implementation.**\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Review this requirements document\n",
    "2. Run `01_Setup_Environment.ipynb` to create data\n",
    "3. Implement Bronze layer transformations\n",
    "4. Implement Silver layer transformations\n",
    "5. Implement Gold layer aggregations\n",
    "6. **Register tables for dashboards** (see `06_Register_Tables_for_Dashboards.ipynb`)\n",
    "7. Use validation checkpoints to verify each layer\n",
    "8. Compare your results with the reference notebooks\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_Requirements_and_Guidelines",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
